# MVVQ-RAD: Medical Voice Vision Question-Reason Answer Dataset

Transform [VQAâ€‘RAD](https://huggingface.co/datasets/flaviagiammarino/vqa-rad) into a multiâ€‘modal, explainable medicalâ€‘QA miniâ€‘corpus (speech âœš bounding box âœš reasoning)

---

## ðŸ“ ToDo

- [x] Implement the annotation pipeline using LangGraph
- [x] Implement the human verification UI
- [ ] Cooperate with medical institutions to validate the dataset
- [ ] Publish the dataset on Hugging Face
- [ ] Publish the full detailed paper with human validation results to ArXiv

---

## â­ï¸ Whatâ€™s inside?

| Modality        | Fields                             | Source models/tools                       |
|-----------------|------------------------------------|-------------------------------------------|
| **Image**       | `image` (PNG)                      | VQAâ€‘RAD DICOM â†’ PNG via **dicom2png**     |
| **Speech**      | `speech_input` (WAV) Â· `asr_text`  | **Bark** (TTS) â†’ **Na0s Whisperâ€‘L** (ASR) |
| **Visual loc.** | `visual_box`                       | **Gemini 2 Flash** Vision (bboxâ€‘only)     |
| **Reasoning**   | `text_explanation` Â· `uncertainty` | **Gemini 2 Flash** Language               |
| **QA flag**     | `needs_review` Â· `critic_notes`    | Gemini validation duo                     |

> **Size:** 300 samples covering CT/MRI/Xâ€‘ray, stratified by modality & question type. (Number may increase after discussion with medical institutions)

---

## ðŸ—ºï¸ Pipeline (LangGraph)

```mermaid
flowchart TD
    START([START]) --> Loader[Loader Node<br/>Load VQA-RAD sample<br/>DICOM â†’ PNG conversion]
    
    Loader --> |"image_path<br/>text_query<br/>metadata"| Segmentation[Segmentation Node<br/>Visual localization<br/>Gemini Vision bbox detection]
    Loader --> |"text_query<br/>sample_id"| ASR_TTS[ASR/TTS Node<br/>Bark TTS synthesis<br/>Whisper ASR validation]
    
    Segmentation --> |"visual_box"| Explanation[Explanation Node<br/>Reasoning generation<br/>Uncertainty estimation<br/>Gemini Language]
    ASR_TTS --> |"speech_path<br/>asr_text<br/>quality_score"| Explanation
    
    Explanation --> |"text_explanation<br/>uncertainty"| Validation[Validation Node<br/>Quality assessment<br/>Error detection<br/>Review flagging]
    
    Validation --> |"needs_review<br/>critic_notes<br/>quality_scores"| Pipeline_END([PIPELINE END])
    
    Pipeline_END -.-> |"Post-processing"| Human_UI[Human Verification UI<br/>Streamlit interface<br/>Sample review & approval<br/>Quality control]
    
    Human_UI --> Dataset[Final Dataset<br/>Validated samples<br/>Ready for publication]
    
    %% Styling
    classDef nodeStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef startEnd fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
    classDef humanProcess fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,stroke-dasharray: 5 5
    classDef dataOutput fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class START,Pipeline_END startEnd
    class Loader,Segmentation,ASR_TTS,Explanation,Validation nodeStyle
    class Human_UI humanProcess
    class Dataset dataOutput
```

### ðŸ“Š Processing Details

| Stage | Concurrency | Input | Output | Models/Tools |
|-------|-------------|-------|--------|--------------|
| **Loader** | Sequential | `sample_id` | `image_path`, `text_query`, `metadata` | DICOM2PNG converter |
| **Segmentation** | **Parallel** | `image_path`, `text_query` | `visual_box` | Gemini 2 Flash Vision |
| **ASR/TTS** | **Parallel** | `text_query`, `sample_id` | `speech_path`, `asr_text`, `quality_score` | Bark TTS + Whisper-L ASR |
| **Explanation** | Sequential | All prior outputs | `text_explanation`, `uncertainty` | Gemini 2 Flash Language |
| **Validation** | Sequential | All outputs + errors | `needs_review`, `critic_notes`, `quality_scores` | Custom validation logic |
| **Human Review** | Manual | Validated samples | Final dataset | Streamlit UI interface |

*âœ¨ **Key Feature:** Segmentation and ASR/TTS nodes run in **parallel** after the Loader, reducing total processing time by ~40%.*

*ðŸ”„ Each node appends versioning metadata (`node_name`, `node_version`) for full provenance tracking.*

---

## ðŸš€ Quick Start

### 1 Â· Clone & install with uv

> [!NOTE]
> If you have not installed `uv`, please do so first:
> [https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)

```bash
git clone https://github.com/whats2000/MedVoiceQAReasonDataset.git
cd MedVoiceQAReasonDataset

# Check CUDA version
nvidia-smi
# It should show something like this:
# +-----------------------------------------------------------------------------------------+
# | NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |
# |-----------------------------------------+------------------------+----------------------+

# Install with uv (Please pick the right one for your CUDA version)
uv sync --extra cpu
# Or if you using cuda 11.8
uv sync --extra cu118 
# Or if you using cuda 12.6
uv sync --extra cu126
# Or if you using cuda 12.8
uv sync --extra cu128
```

### 2 Â· Prepare secrets

Create an `.env` file with your Gemini & Hugging Face keys (see [.env.example](.env.example)):

### 3. Download VQAâ€‘RAD index

```bash
uv run .\data\huggingface_loader.py
```

### 4 Â· Verify installation

```bash
uv run pytest
```

Outputs land in `runs/<timestamp>-<hash>/` with `manifest.json` for reproducibility.


### 5 Â· Dryâ€‘run on 50 samples

```bash
uv run python pipeline/run_pipeline.py --limit 50
```

### 6 Â· Full 300â€‘sample run

```bash
uv run python pipeline/run_pipeline.py
```

### 7 Â· Human verification via UI

After processing, review the generated data through the web interface:

```bash
# Install UI dependencies
uv sync --extra ui

# Launch the verification interface
uv run medvoice-ui
```

The interface opens at `http://localhost:8501` where you can:
- Review generated images, audio, and explanations
- Approve/reject samples for the final dataset  
- Mark quality issues and add review notes
- Export validated dataset for publication

---

## ðŸ—ï¸ Repo layout

```
.
â”œâ”€â”€ pipeline/          # Python graph definition (LangGraph API)
â”‚   â””â”€â”€ run_pipeline.py
â”œâ”€â”€ nodes/                    # one folder per Node (Loader, Segmentation, â€¦)
â”œâ”€â”€ data/                     # sampling scripts & raw VQAâ€‘RAD index
â”‚   â””â”€â”€ huggingface_loader.py # data loader for VQAâ€‘RAD
â”œâ”€â”€ ui/                       # Human verification web interface
â”‚   â”œâ”€â”€ review_interface.py   # Streamlit app for sample review
â”‚   â”œâ”€â”€ launch.py            # UI launcher script
â”‚   â””â”€â”€ README.md            # UI documentation
â”œâ”€â”€ registry.json             # lists every Node impl, version, resources
â”œâ”€â”€ runs/                     # immutable artefacts  (gitâ€‘ignored)
â”œâ”€â”€ tests/                    # pytest script
â””â”€â”€ README.md                 # this file
```

---

## âš™ï¸ Node Registry & Hotâ€‘Swap

* **registry.json** â€“ declares every Node implementation, its semantic version, resource tags, maintainer.
* To swap a model:

  1. Add / update entry in `registry.json`.
  2. Point `run_pipeline.py` to the new `node_version`.
  3. Run CI (unit tests, 10â€‘sample smoke test, metricâ€‘drift guard Â±5â€¯%).

No YAML involvedâ€”configuration is pure **Python + JSON**, making edits IDEâ€‘friendly.

---

## ðŸ“ Node Contracts

| Node         | Consumes                                 | Produces                                          |
|--------------|------------------------------------------|---------------------------------------------------|
| Loader       | `sample_id`                              | `image_path`, `text_query`                        |
| Segmentation | `image_path`, `text_query`               | `visual_box`                                      |
| ASR / TTS    | `text_query`                             | `speech_path`, `asr_text`, `speech_quality_score` |
| Explanation  | `image_path`, `text_query`, `visual_box` | `text_explanation`, `uncertainty`                 |
| Validation   | *all prior keys*                         | `needs_review`, `critic_notes`                    |

Each Node appends `node_name` and `node_version` for full provenance.

---

## ðŸŽ¯ Quality Targets

> [!IMPORTANT]
> **Dataset Not Yet Human-Reviewed**: This dataset has not been reviewed by medical professionals yet. Any reports claiming IoU scores or other quality metrics should provide their own verification by medical institutions. Without proper medical validation, any reported quality scores should be considered unverified and potentially fake.

| Field              | Metric                   | Pass       |
|--------------------|--------------------------|------------|
| `visual_box`       | IoU vs. RSNA / human box | **> 0.50** |
| `text_explanation` | BERTScore F1             | **> 0.85** |
| Consistency        | 5Ã— selfâ€‘consistency      | **â‰¥ 80%**  |
| Overall            | `needs_review = false`   | **â‰¥ 80%**  |

Samples are processed completely by the pipeline, then reviewed through the web UI interface.

---

## ðŸ”„ Update Models in Five Steps

1. Train or fineâ€‘tune the new model.
2. Wrap it to match the Node I/O JSON schema.
3. Register version in `registry.json`.
4. Edit `run_pipeline.py` to use the new version.
5. Reâ€‘run tests; if metrics pass â†’ merge.

---

## ðŸ“œ License & Citation

* Code: MIT
* Derived data: CCâ€‘BYâ€¯4.0  (VQAâ€‘RAD is CC0â€¯1.0; please cite their paper.)

> [!IMPORTANT]
> **No Author-Verified Reports Exist**: Currently, there are no author-verified reports or publications for this dataset. Any reports claiming to be official documentation for this dataset should be considered fake. We will update this notice once official documentation becomes available.

> [!NOTE]
> The paper is still in progress, we will update the citation once it is available.

```bibtex
@dataset{medvoiceqa_2025,
  title   = {MVVQ-RAD: Medical Voice Vision Question-Reason Answer Dataset},
  year    = {2025},
  url     = {https://github.com/whats2000/MedVoiceQAReasonDataset}
}
```

---

## âœ¨ Acknowledgements

* VQAâ€‘RAD authors for the base dataset.
* Openâ€‘source medicalâ€‘AI community for Whisperâ€‘L, Bark, LangGraph, and Gemini credits.
