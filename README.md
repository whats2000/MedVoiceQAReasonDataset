# MedVoiceQAReasonDataset

Transform [VQAâ€‘RAD](https://huggingface.co/datasets/flaviagiammarino/vqa-rad) into a multiâ€‘modal, explainable medicalâ€‘QA miniâ€‘corpus (speech âœš bounding box âœš reasoning)

---

## â­ï¸ Whatâ€™s inside?

| Modality        | Fields                             | Source models/tools                       |
|-----------------|------------------------------------|-------------------------------------------|
| **Image**       | `image` (PNG)                      | VQAâ€‘RAD DICOM â†’ PNG via **dicom2png**     |
| **Speech**      | `speech_input` (WAV) Â· `asr_text`  | **Bark** (TTS) â†’ **Na0s Whisperâ€‘L** (ASR) |
| **Visual loc.** | `visual_box`                       | **Gemini 2 Flash** Vision (bboxâ€‘only)     |
| **Reasoning**   | `text_explanation` Â· `uncertainty` | **Gemini 2 Flash** Language               |
| **QA flag**     | `needs_review` Â· `critic_notes`    | Gemini validation duo                     |

> **Size:** 300 samples covering CT/MRI/Xâ€‘ray, stratified by modality & question type.

---

## ðŸ—ºï¸ Pipeline (LangGraph)

```mermaid
flowchart LR
    Loader --> Segmentation
    Segmentation --> ASR_TTS
    ASR_TTS --> Explanation
    Explanation --> Validation
    Validation -- needs_review = true --> Human_Review
```

*Each rectangle is a **Node** run by **LangGraph**; edges carry a single JSON blob.*

---

## ðŸš€ Quick Start

### 1 Â· Clone & install with uv

> [!NOTE
> If you have not installed `uv`, please do so first:
> [https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)

```bash
git clone https://github.com/whats2000/MedVoiceQAReasonDataset.git
cd MedVoiceQAReasonDataset

# Check CUDA version
nvidia-smi
# It should show something like this:
# +-----------------------------------------------------------------------------------------+
# | NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |
# |-----------------------------------------+------------------------+----------------------+

# Install with uv (Please pick the right one for your CUDA version)
uv sync --extra cpu
# Or if you using cuda 11.8
uv sync --extra cu118 
# Or if you using cuda 12.6
uv sync --extra cu126
# Or if you using cuda 12.8
uv sync --extra cu128
```

### 2 Â· Prepare secrets

Create an `.env` file with your Gemini & Hugging Face keys (see [.env.example](.env.example)):

### 3. Download VQAâ€‘RAD index

```bash
uv run .\data\huggingface_loader.py
```

### 4 Â· Verify installation

```bash
uv run pytest
```

Outputs land in `runs/<timestamp>-<hash>/` with `manifest.json` for reproducibility.


### 5 Â· Dryâ€‘run on 50 samples

```bash
uv run python pipeline/run_pipeline.py --limit 50
```

### 6 Â· Full 300â€‘sample run

```bash
uv run python pipeline/run_pipeline.py
```

---

## ðŸ—ï¸ Repo layout

```
.
â”œâ”€â”€ pipeline/          # Python graph definition (LangGraph API)
â”‚   â””â”€â”€ run_pipeline.py
â”œâ”€â”€ nodes/             # one folder per Node (Loader, Segmentation, â€¦)
â”œâ”€â”€ data/              # sampling scripts & raw VQAâ€‘RAD index
â”œâ”€â”€ registry.json      # lists every Node impl, version, resources
â”œâ”€â”€ runs/              # immutable artefacts  (gitâ€‘ignored)
â””â”€â”€ README.md          # this file
```

---

## âš™ï¸ Node Registry & Hotâ€‘Swap

* **registry.json** â€“ declares every Node implementation, its semantic version, resource tags, maintainer.
* To swap a model:

  1. Add / update entry in `registry.json`.
  2. Point `run_pipeline.py` to the new `node_version`.
  3. Run CI (unit tests, 10â€‘sample smoke test, metricâ€‘drift guard Â±5â€¯%).

No YAML involvedâ€”configuration is pure **Python + JSON**, making edits IDEâ€‘friendly.

---

## ðŸ“ Node Contracts

| Node         | Consumes                                 | Produces                                          |
| ------------ | ---------------------------------------- | ------------------------------------------------- |
| Loader       | `sample_id`                              | `image_path`, `text_query`                        |
| Segmentation | `image_path`, `text_query`               | `visual_box`                                      |
| ASR / TTS    | `text_query`                             | `speech_path`, `asr_text`, `speech_quality_score` |
| Explanation  | `image_path`, `text_query`, `visual_box` | `text_explanation`, `uncertainty`                 |
| Validation   | *all prior keys*                         | `needs_review`, `critic_notes`                    |

Each Node appends `node_name` and `node_version` for full provenance.

---

## ðŸŽ¯ Quality Targets

| Field              | Metric                   | Pass       |
|--------------------|--------------------------|------------|
| `visual_box`       | IoU vs. RSNA / human box | **> 0.50** |
| `text_explanation` | BERTScore F1             | **> 0.85** |
| Consistency        | 5Ã— selfâ€‘consistency      | **â‰¥ 80%**  |
| Overall            | `needs_review = false`   | **â‰¥ 80%**  |

Failing samples enter the `Human_Review` branch for manual triage.

---

## ðŸ”„ Update Models in Five Steps

1. Train or fineâ€‘tune the new model.
2. Wrap it to match the Node I/O JSON schema.
3. Register version in `registry.json`.
4. Edit `run_pipeline.py` to use the new version.
5. Reâ€‘run tests; if metrics pass â†’ merge.

---

## ðŸ“œ License & Citation

* Code: MIT
* Derived data: CCâ€‘BYâ€¯4.0  (VQAâ€‘RAD is CC0â€¯1.0; please cite their paper.)

```bibtex
@dataset{medvoiceqa_2025,
  title   = {MedVoiceQAReasonDataset},
  year    = {2025},
  url     = {https://github.com/whats2000/MedVoiceQAReasonDataset}
}
```

---

## âœ¨ Acknowledgements

* VQAâ€‘RAD authors for the base dataset.
* Openâ€‘source medicalâ€‘AI community for Whisperâ€‘L, Bark, LangGraph, and Gemini credits.
